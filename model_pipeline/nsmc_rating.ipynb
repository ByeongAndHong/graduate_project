{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file = \"./ko_sentencepiece/ko.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_dec_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_dec_seq\": 256,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # \n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob\n",
    "    \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        return ffn_outputs, attn_prob\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, attn_probs\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.dec_enc_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\n",
    "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
    "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_att_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_att_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_enc_att_outputs = self.layer_norm2(self_att_outputs + dec_enc_att_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(dec_enc_att_outputs)\n",
    "        ffn_outputs = self.layer_norm3(dec_enc_att_outputs + ffn_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "    \n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        # (bs, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\n",
    "\n",
    "        self_attn_probs, dec_enc_attn_probs = [], []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq), (bs, n_dec_seq, n_enc_seq)\n",
    "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            self_attn_probs.append(self_attn_prob)\n",
    "            dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)], [(bs, n_dec_seq, n_enc_seq)]S\n",
    "        return dec_outputs, self_attn_probs, dec_enc_attn_probs\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = Transformer(self.config)\n",
    "        self.projection = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)\n",
    "        # (bs, d_hidn)\n",
    "        dec_outputs, _ = torch.max(dec_outputs, dim=1)\n",
    "        # (bs, n_output)\n",
    "        logits = self.projection(dec_outputs)\n",
    "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentences = []\n",
    "\n",
    "        df = pd.read_csv(infile)\n",
    "\n",
    "        rating_mapping = {1: 0, 2: 1, 4: 2, 5: 3}\n",
    "        self.labels = df['rating'].map(rating_mapping).tolist()\n",
    "        \n",
    "        docs = df['document']\n",
    "        for doc in docs:\n",
    "            self.sentences.append([vocab.piece_to_id(p) for p in doc])\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentences)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor([self.vocab.piece_to_id(\"[BOS]\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_collate_fn(inputs):\n",
    "    labels, enc_inputs, dec_inputs = list(zip(*inputs))\n",
    "\n",
    "    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=0)\n",
    "    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels, dim=0),\n",
    "        enc_inputs,\n",
    "        dec_inputs,\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "dataset = MovieDataSet(vocab, './nsmc_data/train.csv')\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, valid_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=movie_collate_fn)\n",
    "\n",
    "valid_dataset = torch.utils.data.Subset(dataset, valid_indices)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)\n",
    "\n",
    "test_dataset = MovieDataSet(vocab, './nsmc_data/test.csv')\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177300, 19700, 3000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(config, model, data_loader):\n",
    "    squared_errors = []\n",
    "    model.eval()\n",
    "\n",
    "    for i, value in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Evaluation\", leave=False):\n",
    "        labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        logits = outputs[0]\n",
    "        _, indices = logits.max(1)\n",
    "\n",
    "        indices = torch.where(indices == 3, 5, indices)\n",
    "        indices = torch.where(indices == 2, 4, indices)\n",
    "        indices = torch.where(indices == 1, 2, indices)\n",
    "        indices = torch.where(indices == 0, 1, indices)\n",
    "\n",
    "\n",
    "        labels = torch.where(labels == 3, 5, labels)\n",
    "        labels = torch.where(labels == 2, 4, labels)\n",
    "        labels = torch.where(labels == 1, 2, labels)\n",
    "        labels = torch.where(labels == 0, 1, labels)\n",
    "\n",
    "        errors = indices.float() - labels.float()  # 오차 계산\n",
    "        squared_errors.extend(errors.pow(2).cpu())  # 오차 제곱을 리스트에 추가\n",
    "\n",
    "    rmse = torch.sqrt(torch.mean(torch.tensor(squared_errors)))  # 평균 제곱근 오차 계산\n",
    "    return rmse.item()  # RMSE 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config, epoch, model, criterion, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    for i, value in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\", leave=False):\n",
    "        labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss_val = loss.item()\n",
    "        losses.append(loss_val)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cpu'), 'n_output': 4}\n"
     ]
    }
   ],
   "source": [
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.n_output = 4 # 1,2,4,5\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MovieClassification(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): Encoder(\n",
       "      (enc_emb): Embedding(8007, 256)\n",
       "      (pos_emb): Embedding(257, 256)\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (dec_emb): Embedding(8007, 256)\n",
       "      (pos_emb): Embedding(257, 256)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dec_enc_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dec_enc_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dec_enc_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dec_enc_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dec_enc_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dec_enc_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (scaled_dot_attn): ScaledDotProductAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (pos_ffn): PoswiseFeedForwardNet(\n",
       "            (conv1): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Linear(in_features=256, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MovieClassification(config)\n",
    "model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train loss : 0.9151266790055609, valid rmse : 1.1724416017532349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, train loss : 0.8400273310587691, valid rmse : 1.1327160596847534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2, train loss : 0.8112697686467852, valid rmse : 1.0990766286849976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3, train loss : 0.7855242374655488, valid rmse : 1.0867218971252441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4, train loss : 0.7645507322417365, valid rmse : 1.0594511032104492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5, train loss : 0.7465931725450409, valid rmse : 1.052480697631836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 6, train loss : 0.729483904174568, valid rmse : 1.082111120223999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 7, train loss : 0.7151707044196507, valid rmse : 1.0668517351150513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 8, train loss : 0.6995576479046204, valid rmse : 1.0514674186706543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 9, train loss : 0.6859905357441689, valid rmse : 1.0609592199325562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10, train loss : 0.6722715997592711, valid rmse : 1.0467256307601929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 11, train loss : 0.6570174840062556, valid rmse : 1.0402557849884033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 12, train loss : 0.6416769354331373, valid rmse : 1.0543359518051147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 13, train loss : 0.6270587456346762, valid rmse : 1.0340644121170044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 14, train loss : 0.6118456571219115, valid rmse : 1.0519258975982666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 15, train loss : 0.5984181372548251, valid rmse : 1.0819000005722046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 16, train loss : 0.5829426982530096, valid rmse : 1.0562119483947754\n",
      ">>>> epoch = 13, loss = 0.62706, rmse = 1.03406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> test rmse = 1.015053391456604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_epoch, best_loss, best_rmse = 0, 0, float('inf')\n",
    "losses, rmses = [], []\n",
    "cnt=0\n",
    "for epoch in range(n_epoch):\n",
    "    loss = train_epoch(config, epoch, model, criterion, optimizer, train_loader)\n",
    "    rmse = eval_epoch(config, model, valid_loader)\n",
    "\n",
    "    losses.append(loss)\n",
    "    rmses.append(rmse)\n",
    "    print(f'epoch : {epoch}, train loss : {loss}, valid rmse : {rmse}')\n",
    "\n",
    "    if best_rmse > rmse:\n",
    "        best_epoch, best_loss, best_rmse = epoch, loss, rmse\n",
    "        torch.save(model.state_dict(), 'nsmc_rating.pth')\n",
    "        cnt = 0\n",
    "    else:\n",
    "        cnt += 1\n",
    "        if cnt == 3:\n",
    "            break\n",
    "    \n",
    "print(f\">>>> epoch = {best_epoch}, loss = {best_loss:.5f}, rmse = {best_rmse:.5f}\")\n",
    "test_rmse = eval_epoch(config, model, test_loader)\n",
    "print(f'>>>> test rmse = {test_rmse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel = MovieClassification(config)\n",
    "testmodel.cpu()\n",
    "testmodel.load_state_dict(torch.load('nsmc_rating.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7497, 7065, 6856, 6858, 6902,    1, 7126, 6878,    1, 6915, 6946, 6873,\n",
      "         6861, 6858, 7067, 6893, 6874,    1, 6856, 6902, 6858,    1,    1,    1,\n",
      "         6939, 6864, 6860, 7768, 7768, 7768]])\n",
      "tensor([[2]])\n",
      "tensor([1])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "testmodel.eval()\n",
    "test_input = \"딸랑이는거 별로 안좋아하는편인데 이거는 굿굿입니다~~~\"\n",
    "test_enc_input = torch.tensor([[vocab.piece_to_id(p) for p in test_input]])\n",
    "test_dec_input = torch.tensor([[vocab.piece_to_id(\"[BOS]\")]])\n",
    "\n",
    "print(test_enc_input)\n",
    "print(test_dec_input)\n",
    "\n",
    "test_output = testmodel(test_enc_input, test_dec_input)\n",
    "test_logit = test_output[0]\n",
    "_, predicted = torch.max(test_logit, 1)\n",
    "print(predicted)\n",
    "\n",
    "if predicted.item()==0:\n",
    "    print(1)\n",
    "elif predicted.item()==1:\n",
    "    print(2)\n",
    "elif predicted.item()==2:\n",
    "    print(4)\n",
    "else:\n",
    "    print(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_epoch(config, model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    mismatched_samples = []\n",
    "    er0, er1, er2 = 0, 0, 0\n",
    "\n",
    "    for i, value in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"compare\", leave=False):\n",
    "        labels, enc_inputs, dec_inputs = map(lambda v: v.to('cpu'), value)\n",
    "\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        logits = outputs[0]\n",
    "        _, indices = logits.max(1)\n",
    "\n",
    "        #pred\n",
    "        indices = torch.where(indices == 3, 5, indices)\n",
    "        indices = torch.where(indices == 2, 4, indices)\n",
    "        indices = torch.where(indices == 1, 2, indices)\n",
    "        indices = torch.where(indices == 0, 1, indices)\n",
    "\n",
    "        labels = torch.where(labels == 3, 5, labels)\n",
    "        labels = torch.where(labels == 2, 4, labels)\n",
    "        labels = torch.where(labels == 1, 2, labels)\n",
    "        labels = torch.where(labels == 0, 1, labels)\n",
    "\n",
    "        mismatched_differences = torch.abs(indices - labels)  # 예측값과 실제값의 차이를 절댓값으로 계산\n",
    "        num_difference_0 = torch.sum(mismatched_differences == 0).item()  # 차이가 0인 개수\n",
    "        num_difference_1 = torch.sum(mismatched_differences == 1).item()  # 차이가 1인 개수\n",
    "        num_difference_2_plus = torch.sum(mismatched_differences >= 2).item()  # 차이가 2 이상인 개수\n",
    "\n",
    "        er0 += num_difference_0\n",
    "        er1 += num_difference_1\n",
    "        er2 += num_difference_2_plus\n",
    "\n",
    "        # indices와 labels이 다른 원소별로 enc_inputs, indices, labels 저장\n",
    "        mismatched_indices = torch.where(indices != labels)[0]\n",
    "        for idx in mismatched_indices:\n",
    "            mismatched_samples.append((enc_inputs[idx], indices[idx], labels[idx]))\n",
    "\n",
    "\n",
    "    return mismatched_samples, er0, er1, er2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "mismatch, er0, er1, er2 = compare_epoch(config, testmodel, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er0 : 2010\n",
      "er1 : 724\n",
      "er2 : 266\n"
     ]
    }
   ],
   "source": [
    "print(f'er0 : {er0}')\n",
    "print(f'er1 : {er1}')\n",
    "print(f'er2 : {er2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 7, 14, 19, 27, 28, 29, 30, 31, 32, 38, 40, 49, 50, 54, 56, 57, 59, 60, 66, 70, 71, 73, 75, 78, 79, 81, 82, 84, 87, 88, 90, 91, 92, 95, 108, 111, 114, 115, 119, 123, 125, 126, 129, 130, 131, 133, 134, 135, 137, 142, 144, 145, 147, 151, 155, 159, 162, 163, 169, 170, 172, 174, 177, 180, 182, 184, 188, 191, 195, 196, 197, 198, 201, 203, 212, 213, 214, 215, 216, 217, 221, 222, 229, 230, 232, 235, 236, 238, 245, 247, 255, 257, 264, 266, 267, 268, 270, 271, 272, 273, 274, 276, 283, 284, 286, 289, 290, 291, 292, 293, 294, 296, 299, 301, 304, 305, 306, 309, 310, 311, 324, 328, 329, 331, 333, 335, 337, 338, 340, 341, 343, 346, 353, 357, 360, 362, 372, 377, 379, 381, 385, 387, 390, 391, 392, 393, 396, 400, 409, 414, 415, 416, 417, 418, 423, 425, 426, 428, 433, 434, 435, 438, 439, 447, 448, 452, 456, 459, 468, 469, 471, 473, 474, 477, 480, 481, 483, 488, 489, 493, 494, 495, 497, 499, 507, 509, 514, 515, 522, 524, 532, 533, 535, 538, 540, 542, 543, 546, 548, 551, 559, 561, 562, 567, 568, 570, 573, 574, 575, 578, 580, 581, 582, 587, 588, 590, 596, 599, 600, 602, 604, 605, 606, 613, 614, 615, 617, 618, 620, 622, 624, 628, 629, 630, 631, 632, 637, 641, 643, 644, 646, 647, 649, 650, 657, 659, 663, 667, 669, 670, 672, 675, 677, 678, 679, 697, 701, 703, 704, 705, 706, 716, 718, 726, 728, 730, 736, 742, 746, 747, 749, 750, 752, 757, 758, 759, 760, 761, 762, 765, 772, 773, 774, 776, 777, 778, 779, 780, 781, 782, 783, 784, 786, 790, 792, 793, 794, 798, 799, 801, 802, 803, 805, 806, 807, 809, 812, 814, 816, 817, 818, 826, 831, 832, 833, 835, 836, 838, 840, 848, 855, 856, 860, 867, 869, 870, 871, 872, 873, 874, 883, 885, 886, 887, 888, 894, 895, 896, 901, 905, 906, 907, 908, 909, 910, 914, 917, 921, 922, 926, 929, 931, 934, 935, 938, 940, 944, 946, 947, 951, 952, 955, 956, 958, 960, 962, 967, 969, 971, 972, 973, 976, 979, 981, 988]\n",
      "[2, 6, 9, 17, 24, 34, 35, 37, 41, 48, 51, 61, 63, 69, 72, 85, 86, 96, 97, 102, 104, 105, 107, 110, 113, 116, 122, 136, 140, 143, 148, 150, 156, 160, 165, 166, 167, 171, 173, 178, 181, 186, 190, 192, 193, 199, 200, 207, 208, 211, 218, 219, 225, 227, 228, 239, 241, 242, 243, 248, 252, 256, 275, 278, 280, 282, 288, 302, 308, 315, 326, 327, 330, 339, 347, 348, 349, 350, 361, 363, 364, 366, 383, 388, 394, 395, 404, 406, 407, 408, 410, 411, 422, 427, 450, 460, 461, 464, 472, 478, 486, 487, 491, 492, 498, 500, 503, 508, 511, 513, 519, 521, 531, 547, 552, 553, 555, 556, 558, 560, 563, 564, 569, 571, 576, 583, 586, 594, 597, 610, 616, 621, 625, 626, 634, 635, 648, 651, 660, 671, 673, 674, 685, 686, 687, 688, 691, 695, 708, 710, 711, 715, 717, 724, 729, 741, 748, 751, 756, 766, 767, 775, 785, 787, 791, 804, 815, 830, 842, 843, 845, 846, 847, 849, 851, 852, 853, 857, 863, 864, 879, 881, 891, 893, 898, 900, 902, 911, 913, 918, 919, 920, 924, 930, 937, 939, 941, 950, 954, 957, 964, 975, 977, 980, 983, 986, 987]\n",
      "[0, 1, 3, 8, 11, 15, 16, 20, 21, 22, 23, 25, 26, 33, 39, 45, 46, 47, 55, 58, 62, 65, 67, 74, 76, 77, 83, 98, 99, 100, 103, 112, 117, 118, 120, 127, 128, 132, 139, 141, 146, 152, 153, 154, 157, 161, 164, 168, 175, 176, 179, 183, 185, 187, 189, 194, 204, 205, 206, 209, 210, 220, 223, 224, 226, 231, 234, 237, 240, 244, 249, 251, 253, 258, 260, 261, 263, 265, 269, 279, 295, 297, 298, 300, 303, 307, 312, 317, 321, 322, 336, 344, 345, 352, 358, 359, 365, 368, 370, 371, 374, 375, 376, 380, 382, 386, 389, 397, 401, 403, 412, 420, 424, 431, 432, 436, 437, 440, 441, 442, 443, 444, 445, 446, 457, 458, 462, 463, 465, 466, 467, 470, 475, 479, 484, 485, 501, 504, 510, 512, 516, 518, 520, 525, 526, 527, 530, 534, 536, 539, 541, 545, 554, 557, 565, 572, 577, 579, 584, 598, 603, 608, 609, 623, 627, 633, 636, 640, 642, 645, 652, 654, 656, 658, 661, 662, 665, 666, 668, 676, 680, 681, 682, 693, 696, 698, 707, 709, 712, 713, 719, 722, 725, 727, 731, 732, 733, 735, 737, 738, 739, 744, 745, 753, 754, 755, 763, 768, 771, 788, 789, 795, 796, 797, 800, 810, 811, 813, 819, 820, 821, 822, 823, 825, 827, 837, 839, 841, 844, 850, 854, 858, 862, 865, 866, 868, 876, 877, 880, 882, 884, 889, 890, 897, 899, 903, 912, 915, 923, 927, 932, 933, 936, 943, 948, 949, 953, 959, 963, 974, 982, 984, 985, 989]\n",
      "[10, 12, 13, 18, 36, 42, 43, 44, 52, 53, 64, 68, 80, 89, 93, 94, 101, 106, 109, 121, 124, 138, 149, 158, 202, 233, 246, 250, 254, 259, 262, 277, 281, 285, 287, 313, 314, 316, 318, 319, 320, 323, 325, 332, 334, 342, 351, 354, 355, 356, 367, 369, 373, 378, 384, 398, 399, 402, 405, 413, 419, 421, 429, 430, 449, 451, 453, 454, 455, 476, 482, 490, 496, 502, 505, 506, 517, 523, 528, 529, 537, 544, 549, 550, 566, 585, 589, 591, 592, 593, 595, 601, 607, 611, 612, 619, 638, 639, 653, 655, 664, 683, 684, 689, 690, 692, 694, 699, 700, 702, 714, 720, 721, 723, 734, 740, 743, 764, 769, 770, 808, 824, 828, 829, 834, 859, 861, 875, 878, 892, 904, 916, 925, 928, 942, 945, 961, 965, 966, 968, 970, 978]\n",
      "990\n"
     ]
    }
   ],
   "source": [
    "cnt1 = []\n",
    "cnt2 = []\n",
    "cnt4 = []\n",
    "cnt5 = []\n",
    "for i in range(len(mismatch)):\n",
    "    if mismatch[i][2].item()==4:\n",
    "        cnt4.append(i)\n",
    "    elif mismatch[i][2].item()==5:\n",
    "        cnt5.append(i)\n",
    "    elif mismatch[i][2].item()==1:\n",
    "        cnt1.append(i)\n",
    "    elif mismatch[i][2].item()==2:\n",
    "        cnt2.append(i)\n",
    "\n",
    "print(cnt1)\n",
    "print(cnt2)\n",
    "print(cnt4)\n",
    "print(cnt5)\n",
    "print(len(cnt1)+len(cnt2)+len(cnt4)+len(cnt5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딸랑이는거 ⁇ 별로 ⁇ 안좋아하는편인데 ⁇ 이거는 ⁇  ⁇  ⁇ 입니다~~~\n",
      "pred : 2\n",
      "label : 5\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "print(vocab.DecodeIds(mismatch[idx][0].tolist()))\n",
    "print(f'pred : {mismatch[idx][1]}')\n",
    "print(f'label : {mismatch[idx][2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
